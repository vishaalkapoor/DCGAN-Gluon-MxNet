{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN implemented on MXNet Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import math\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites for Scripting\n",
    "\n",
    "1) Install Pillow\n",
    "\n",
    "!pip install pillow\n",
    "\n",
    "2) Obtain the Dataset via the Google Drive link:\n",
    "https://drive.google.com/open?id=0B7EVK8r0v71pZjFTYXZWM3FlRnM\n",
    "\n",
    "Website is at http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html.\n",
    "\n",
    "3) Extract the zip file to datasets\n",
    "\n",
    "```\n",
    " cd SageMaker/face_generation\n",
    " mkdir dataset\n",
    " cd dataset\n",
    " unzip ../img_align_celeba.zip\n",
    " \n",
    "```\n",
    "\n",
    "4) Make an output directory\n",
    "```\n",
    "mkdir output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 64     # input batch size\n",
    "imageSize = 64     # the height / width of the input image to network'\n",
    "nz = 100           # size of the latent z vector (tbe \"noise\" dimension)\n",
    "ngf = 64\n",
    "ndf = 64 \n",
    "nc = 3             # number of color channels RGB\n",
    "niter = 125 # number of epochs to train for\n",
    "\n",
    "lr = 0.0001 # learning rate, default=0.0002\n",
    "\n",
    "beta1 = 0.5 # beta1 for adam\n",
    "beta2 = 0.999 # beta2 for adam\n",
    "\n",
    "ctx = mx.cpu()      # the context gpu/cpu\n",
    "\n",
    "clip_gradient = 10.0\n",
    "\n",
    "manualSeed = random.randint(1, 10000) # manual seed \n",
    "mx.random.seed(manualSeed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(data, label):\n",
    "    data = mx.image.imresize(data, imageSize, imageSize)\n",
    "    data = mx.nd.transpose(data, (2,0,1))\n",
    "    data = data.astype(np.float32)/128.0-1.0 \n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a data iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataroot = './dataset'\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(dataroot,transform=transformer),\n",
    "    batch_size= batchSize, shuffle=True, last_batch='discard')\n",
    "\n",
    "test_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.ImageFolderDataset(dataroot, transform=transformer),\n",
    "    batch_size=batchSize, shuffle=False, last_batch='discard')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create the Generator and Discriminator networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_Net = gluon.nn.Sequential()\n",
    "with G_Net.name_scope():\n",
    "    # First layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=ngf * 8, kernel_size=4, use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1, center=True))\n",
    "    G_Net.add(gluon.nn.Activation('relu'))\n",
    "    # Second layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=ngf * 4, kernel_size=4, strides = 2, padding=1, use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1, center=True))\n",
    "    G_Net.add(gluon.nn.Activation('relu'))\n",
    "    \n",
    "    # Third layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=ngf * 2, kernel_size=4, strides=2,\n",
    "                                       padding=1, use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1, center=True))\n",
    "    G_Net.add(gluon.nn.Activation('relu'))\n",
    "    # Fourth layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=ngf, kernel_size=4, strides = 2, padding=1, use_bias=False))\n",
    "    G_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1, center=True))\n",
    "    G_Net.add(gluon.nn.Activation('relu'))\n",
    "    # Fifth layer\n",
    "    G_Net.add(gluon.nn.Conv2DTranspose(channels=nc, kernel_size=4, strides=2, padding=1, use_bias=False))\n",
    "    G_Net.add(gluon.nn.Activation('tanh'))\n",
    "\n",
    "    \n",
    "    \n",
    "D_Net = gluon.nn.Sequential()\n",
    "with D_Net.name_scope():\n",
    "    # First layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=ndf, kernel_size=4, strides=2, padding=1, use_bias=False))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    \n",
    "    # Second layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=ndf*2, kernel_size=4, strides=2, padding=1, use_bias=False))\n",
    "    D_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1, center=True))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    \n",
    "    # Third layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=ndf*4, kernel_size=4,strides=2,padding=1,use_bias=False))\n",
    "    D_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    \n",
    "    # Fourth layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=ndf*8, kernel_size=4, strides=2, padding=1, use_bias=False))\n",
    "    D_Net.add(gluon.nn.BatchNorm(axis = 1, momentum = 0.1,center=True))\n",
    "    D_Net.add(gluon.nn.LeakyReLU(0.2))\n",
    "    \n",
    "    # Fifth layer\n",
    "    D_Net.add(gluon.nn.Conv2D(channels=1, kernel_size=4, strides=2, padding=0, use_bias=False))\n",
    "    D_Net.add(gluon.nn.Activation('sigmoid'))\n",
    "    \n",
    "g_net = G_Net\n",
    "d_net = D_Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A function to visualizing the created and real images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outf = './output'\n",
    "def image_show(data,padding = 2):\n",
    "    datanp = np.clip((data.asnumpy().transpose((0, 2, 3, 1)) + 1.0)*128.0,0,255).astype(np.uint8)\n",
    "    \n",
    "    x_dim = min(8, batchSize)\n",
    "    y_dim = int(math.ceil(float(batchSize) / x_dim))\n",
    "    height, width = int(imageSize + padding), int(imageSize + padding)\n",
    "    grid = np.zeros(( height * y_dim + 1 + padding // 2, width * x_dim + 1 + padding // 2,3),dtype = np.uint8)\n",
    "    k = 0\n",
    "    for y in range(y_dim):\n",
    "        for x in range(x_dim):\n",
    "            if k >= batchSize:\n",
    "                break\n",
    "            start_y = y * height + 1 + padding // 2\n",
    "            end_y = start_y + height - padding \n",
    "            start_x = x * width + 1 + padding // 2\n",
    "            end_x = start_x + width - padding\n",
    "            np.copyto(grid[start_y:end_y,start_x:end_x,:],datanp[k])\n",
    "            k = k + 1\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(grid)\n",
    "    plt.show()\n",
    "    scipy.misc.imsave('%s/fake_samples_epoch_%03d.png' % (outf, epoch),grid)\n",
    "#    plt.savefig('%s/%s/fake_samples_epoch_%03d.png' % (outf,dataset, epoch))\n",
    "def binary_cross_entropy(yhat, y):\n",
    "    return - (y * nd.log(yhat) + (1 - y ) * nd.log(1 - yhat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the networks and the optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialization\n",
    "\n",
    "g_net.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "d_net.collect_params().initialize(mx.init.Normal(0.02), ctx=ctx)\n",
    "G_trainer = gluon.Trainer(g_net.collect_params(), 'Adam', {'learning_rate': lr * 10,'beta1':beta1,'beta2':beta2,'clip_gradient':clip_gradient})\n",
    "D_trainer = gluon.Trainer(d_net.collect_params(), 'Adam', {'learning_rate': lr,'beta1':beta1,'beta2':beta2,'clip_gradient':clip_gradient})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g_net.collect_params().zero_grad()\n",
    "d_net.collect_params().zero_grad()\n",
    "counter = 0\n",
    "for epoch in range(niter):\n",
    "    for i, (d, _ ) in enumerate(train_data):\n",
    "        # Update Discriminator\n",
    "        data = d.as_in_context(ctx)\n",
    "        label = nd.ones(batchSize, ctx)\n",
    "        noise = nd.normal(loc = 0, scale = 1, shape = (batchSize, nz,1,1),ctx = ctx)\n",
    "        with autograd.record():            \n",
    "            output = d_net(data)\n",
    "            D_error = nd.mean(binary_cross_entropy(output,label))\n",
    "            D_x = nd.mean(output)\n",
    "            fake_image = g_net(noise)\n",
    "            output = d_net(fake_image.detach())\n",
    "            D_error_fake_image = nd.mean(binary_cross_entropy(output,label*0))\n",
    "            D_G_z1 = nd.mean(output)\n",
    "            D_e = D_error + D_error_fake_image\n",
    "            D_e.backward()\n",
    "        D_trainer.step(batchSize)\n",
    "        \n",
    "        # Update Generator\n",
    "        label = nd.ones(batchSize, ctx)\n",
    "        with autograd.record():\n",
    "            fake_image = g_net(noise)\n",
    "            output = d_net(fake_image)\n",
    "            G_error = nd.mean(binary_cross_entropy(output,label))\n",
    "            D_G_z2 = nd.mean(output)\n",
    "            G_error.backward()\n",
    "        G_trainer.step(batchSize)\n",
    "        \n",
    "        \n",
    "        print('[%d/%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch,counter, niter, i, len(train_data),\n",
    "                 D_error.asnumpy(), G_error.asnumpy(), D_x.asnumpy(), D_G_z1.asnumpy(), D_G_z2.asnumpy()))\n",
    "        trunctate = 1\n",
    "        if i % (100 * trunctate) == 0:\n",
    "            image_show(data)\n",
    "            image_show(fake_image)\n",
    "        if i % (1000 * trunctate) == 0: \n",
    "            #plt.savefig('%s/%s/fake_samples_epoch_%03d.png' % (outf,dataset, epoch))\n",
    "            filenameG = '%s/G_Net_epoch_%d_%d' % (outf, epoch,counter)\n",
    "            filenameD = '%s/D_Net_epoch_%d_%d' % (outf, epoch,counter)\n",
    "            g_net.save_params(filenameG)\n",
    "            d_net.save_params(filenameD)\n",
    "            counter = counter + 1\n",
    "    counter = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gluon",
   "language": "python",
   "name": "gluon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
